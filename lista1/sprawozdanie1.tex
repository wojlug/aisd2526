\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{polski}
\usepackage[margin=2.5cm]{geometry}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage[svgnames]{xcolor}


\usepackage{listings}
\lstdefinestyle{mojStylCpp}{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.1,0.5,0.1},
    stringstyle=\color[rgb]{0.6,0.1,0.1},
    backgroundcolor=\color[rgb]{0.95,0.95,0.95},
    breaklines=true,
    numbers=left,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    captionpos=b
}
\lstset{style=mojStylCpp}
\usepackage{amsmath}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
}

% ======================================================
%                 DOKUMENT WŁAŚCIWY
% ======================================================

\title{Sprawozdanie z Analizy Algorytmów Sortowania}
\author{Wojciech Ługowski (287292)}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

% ------------------------------------------------------
\section{Wstęp}
% ------------------------------------------------------

W tym sprawozdaniu porównałem 6 algorytmów sortowania, aby sprawdzić, jak różnią się ich szybkością i jak na ich działanie wpływają wprowadzone modyfikacje.

Podzieliłem je na dwie grupy:
\begin{itemize}
    \item Trzy klasyczne algorytmy: Insertion Sort (Sortowanie przez wstawianie), Merge Sort (Sortowanie przez scalanie) oraz Heap Sort (Sortowanie przez kopcowanie).
    \item Trzy autorskie modyfikacje tych algorytmów, mające na celu optymalizację: wstawianie parami (dla Insertion Sort), scalanie trójdrożne (dla Merge Sort) oraz sortowanie z użyciem kopca trójkowego (dla Heap Sort).
\end{itemize}

Wszystkie algorytmy zostały zaimplementowane w języku \textbf{C++}. Podczas testów mierzyłem czas ich wykonania oraz zliczałem kluczowe operacje: porównania elementów (`comp`) i przypisania wartości (`sign`). Testy przeprowadziłem na zbiorach danych o różnej wielkości (od $N=10$ do $N=50000$) oraz o różnym stopniu wstępnego uporządkowania (dane losowe, posortowane rosnąco i posortowane malejąco).

% ------------------------------------------------------
\section{Ciekawe fragmenty kodu}
% ------------------------------------------------------

Bazowe implementacje Insertion Sort, Merge Sort czy Heap Sort są dobrze znane. Poniżej skupiam się na najciekawszych fragmentach kodu dotyczących wprowadzonych modyfikacji.

\subsection{Scalanie Trójdrożne (ModMergeSort)}

W przeciwieństwie do standardowego Merge Sort, który dzieli tablicę na dwie części, moja modyfikacja dzieli ją na trzy. Kluczową częścią jest funkcja MergeThree, która odpowiada za scalanie trzech już posortowanych podtablic w jedną. Proces ten realizowany jest w dwóch etapach:
\begin{enumerate}
    \item Scalenie dwóch pierwszych podtablic do pomocniczej tablicy `temp`.
    \item Scalenie zawartości `temp` z trzecią podtablicą z powrotem do głównej tablicy `A`.
\end{enumerate}

\begin{lstlisting}[language=C++, caption={Fragment implementacji funkcji MergeThree}]
void MergeThree(int A[], int p, int s1, int s2, int k, 
                long long &comp, long long &sign) { 
    int nTemp = s2 - p + 1;
    int *temp = new int[nTemp];
    int i = p, j = s1 + 1, t = 0;
    while (i <= s1 && j <= s2) {
        comp++;
        if (A[i] <= A[j]) {
            temp[t] = A[i];
            sign++;
            i++;
            t++;
        }
    /// ...reszta warunkow dla scalania dwoch pierwszych czesci...   
    /// nastepnie cala zawartosc tablicy temp jest scalana z trzecia czescia (od s2+1 do k)
    /// z powrotem do tablicy glownej A. Dla zachowania czytelnosci pominiento to w liscie.
    delete[] temp;
}
\end{lstlisting}

\subsection{Kopiec Trójkowy (HeapSortTernary)}

Ta modyfikacja polega na zastąpieniu klasycznego kopca binarnego (gdzie każdy węzeł ma co najwyżej dwóch potomków) kopcem trójkowym (gdzie każdy węzeł może mieć do trzech potomków). Skutkuje to "płytszą" strukturą kopca. Wymagało to dostosowania funkcji HeapifyTernary, która teraz musi znajdować największy element spośród aż czterech: rodzica i jego trzech potencjalnych dzieci.

\begin{lstlisting}[language=C++, caption={Fragment implementacji funkcji HeapifyTernary}]
void HeapifyTernary(int A[], int n, int i, long long &comp, long long &sign) {
    int p1 = Part_1(i); // Oblicza indeks pierwszego dziecka
    int p2 = Part_2(i); // Oblicza indeks drugiego dziecka
    int p3 = Part_3(i); // Oblicza indeks trzeciego dziecka
    int largest = i;    // Zakładamy, ze rodzic jest najwiekszy

    if (p1 <= n) {
        comp++;
        if (A[p1] > A[largest]) largest = p1;
    }
    // ... podobne sprawdzenia dla p2 i p3 ...
    // ... jesli jest potrzeba zamiany, wykonujemy ja i rekurencyjnie wywolujemy Heapify ...
    if (largest != i) {
        swap(A[i], A[largest]); sign++;
        HeapifyTernary(A, n, largest, comp, sign);
    }
}
\end{lstlisting}

\newpage
% ------------------------------------------------------
\section{Wyniki i Wykresy}
% ------------------------------------------------------

Ta sekcja przedstawia najważniejsze wyniki eksperymentów w postaci wykresów wygenerowanych w \textbf{LibreOffice Calc} na podstawie zebranych danych.

\subsection{Skalowalność: $O(n^2)$ vs $O(n \log n)$}

Pierwszy wykres doskonale ilustruje fundamentalną różnicę w klasach złożoności algorytmów sortowania, pokazując, jak czas wykonania rośnie wraz z rozmiarem danych $N$ dla danych losowych.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{sprawozdanie/obrazy-spr/A1.png}
    \caption{Czas sortowania dla danych losowych. Algorytmy $O(n^2)$ (czerwony i żółty) wykazują gwałtowny wzrost czasu wykonania.}
    \label{fig:czas_vs_n_losowe}
\end{figure}

Na Rysunku \ref{fig:czas_vs_n_losowe} widać, że InsertionSort i jego modyfikacja (o złożoności $O(n^2)$) rosną kwadratowo, a ich czas wykonania drastycznie zwiększa się z każdym kolejnym rzędem wielkości $N$. Pozostałe algorytmy ($O(n \log n)$), takie jak MergeSort i HeapSort wraz z ich modyfikacjami, są tak wydajne, że ich linie leżą blisko osi X, demonstrując znacznie lepszą skalowalność.

\subsection{Wpływ wstępnego uporządkowania danych}

Charakterystyka początkowego ułożenia danych może mieć ogromny wpływ na wydajność niektórych algorytmów. Poniższe wykresy analizują, jak algorytmy radzą sobie z danymi już posortowanymi (najlepszy przypadek dla Insertion Sort) oraz posortowanymi odwrotnie (najgorszy przypadek).

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{sprawozdanie/obrazy-spr/1.3.png}
    \caption{Czas sortowania dla danych posortowanych rosnąco (najlepszy przypadek dla InsertionSort).}
    \label{fig:dane_posortowane}
\end{figure}

Rysunek \ref{fig:dane_posortowane} ukazuje niezwykłą wydajność InsertionSort dla danych już posortowanych. Dzięki temu, że jego złożoność w najlepszym przypadku wynosi $O(n)$, stał się on najszybszym algorytmem ze wszystkich w tym scenariuszu.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{sprawozdanie/obrazy-spr/1.4.png}
    \caption{Czas sortowania dla danych posortowanych malejąco (najgorszy przypadek dla InsertionSort).}
    \label{fig:dane_odwrotne}
\end{figure}

Z kolei Rysunek \ref{fig:dane_odwrotne} ilustruje najgorszy przypadek dla InsertionSort – dane posortowane odwrotnie. W tej sytuacji algorytm jest ekstremalnie wolny. Interesującym wnioskiem jest to, że algorytmy MergeSort i HeapSort (i ich modyfikacje) utrzymywały bardzo podobne czasy wykonania, niezależnie od wstępnego uporządkowania danych, co potwierdza ich stabilność.

\subsection{Szczegółowa analiza modyfikacji}

Aby dokładnie ocenić wpływ wprowadzonych modyfikacji, skupiono się na bezpośrednim porównaniu każdej pary algorytmów (oryginał vs modyfikacja) na danych losowych.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{sprawozdanie/obrazy-spr/2.1.png}
    \caption{Porównanie czasów algorytmów $O(n \log n)$ dla danych losowych.}
    \label{fig:tylko_nlogn}
\end{figure}

Rysunek \ref{fig:tylko_nlogn} (będący "zbliżeniem" na algorytmy $O(n \log n)$ z Rysunku \ref{fig:czas_vs_n_losowe}) wyraźnie pokazuje, że obie modyfikacje algorytmów o złożoności $O(n \log n)$ przyniosły poprawę. Zarówno ModMergeSort (linia jasnoniebieska) jak i HeapSortTernary (linia pomarańczowa) wykazują konsekwentnie niższe czasy wykonania niż ich bazowe wersje (MergeSort - granatowa i HeapSort - ciemnozielona).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{sprawozdanie/obrazy-spr/2.22.png}
    \caption{Porównanie czasu wykonania: InsertionSort vs ModInsertionSort dla danych losowych.}
    \label{fig:time_insertion}
\end{figure}

Jak widać na Rysunku \ref{fig:time_insertion}, modyfikacja InsertionSort (sortowanie parami) nie przyniosła niestety żadnego zysku wydajnościowego dla danych losowych. Czasy wykonania obu algorytmów są praktycznie identyczne, co potwierdzają także dane w tabeli \ref{tab:wyniki_n20000}. Dodatkowa złożoność nie przełożyła się na wymierne korzyści.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{sprawozdanie/obrazy-spr/2.3.png}
    \caption{Porównanie czasu wykonania: MergeSort vs ModMergeSort dla danych losowych.}
    \label{fig:time_merge}
\end{figure}

Rysunek \ref{fig:time_merge} wyraźnie pokazuje, że modyfikacja ModMergeSort była konsekwentnie szybsza od standardowej wersji. Ten zysk w czasie jest bardzo obiecujący.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{sprawozdanie/obrazy-spr/3.4.png}
    \caption{Porównanie liczby porównań: MergeSort vs ModMergeSort dla danych losowych.}
    \label{fig:comp_merge}
\end{figure}

Rysunek \ref{fig:comp_merge} wyjaśnia źródło przewagi ModMergeSort: algorytm ten wykonywał minimalnie mniej porównań dla każdej testowanej wielkości $N$. Mniejsza liczba porównań bezpośrednio przekłada się na krótszy czas wykonania.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{sprawozdanie/obrazy-spr/2.4.png}
    \caption{Porównanie czasu wykonania: HeapSort vs HeapSortTernary dla danych losowych.}
    \label{fig:time_heap}
\end{figure}

W przypadku HeapSort (Rysunek \ref{fig:time_heap}), modyfikacja na kopiec trójkowy również okazała się szybsza od wersji binarnej. Podobnie jak przy ModMergeSort, zysk wydajności jest wyraźnie widoczny.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{sprawozdanie/obrazy-spr/3.3.png}
    \caption{Porównanie liczby porównań: HeapSort vs HeapSortTernary dla danych losowych.}
    \label{fig:comp_heap}
\end{figure}

Co ciekawe, Rysunek \ref{fig:comp_heap} pokazuje, że liczba porównań dla obu wersji HeapSort była bardzo zbliżona. Oznacza to, że zysk wydajności kopca trójkowego musiał pochodzić z innego czynnika – jak to zostanie pokazane poniżej, była to drastycznie mniejsza liczba przypisań.

\subsection{Analiza kosztów operacji (porównania vs przypisania)}

Aby dogłębnie zrozumieć, dlaczego algorytmy różniły się wydajnością, przeanalizowano całkowitą liczbę wykonanych operacji porównania (`comp`) oraz przypisania (`sign`) dla testu z $N=20000$ na danych losowych.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{sprawozdanie/obrazy-spr/3.11.png}
    \caption{Profil operacji (porównania i przypisania) dla N=20000 na danych losowych. Oś Y przedstawiona w skali logarytmicznej dla lepszej czytelności.}
    \label{fig:profil_kosztow}
\end{figure}

Rysunek \ref{fig:profil_kosztow} (z osią Y w skali logarytmicznej) jest kluczowy do zrozumienia mechanizmów stojących za zaobserwowanymi czasami. Wyraźnie widać, że:
\begin{itemize}
    \item Algorytmy o złożoności $O(n^2)$ (InsertionSort i ModInsertionSort) wykonały o rzędy wielkości więcej operacji, co potwierdza ich nieefektywność dla dużych danych.
    \item Wśród algorytmów $O(n \log n)$, ModMergeSort wykonał zdecydowanie najmniej porównań, co jest zgodne z teorią, że podział na 3 zmniejsza głębokość rekursji potrzebnej do porównań.
    \item Z kolei HeapSortTernary wykonał znacznie mniej przypisań niż standardowy HeapSort. To sugeruje, że rzadsze przesuwanie danych (dzięki płytszej strukturze kopca trójkowego) skutecznie zrekompensowało fakt, że liczba porównań była zbliżona, prowadząc do ogólnego skrócenia czasu.
\end{itemize}

Poniższa tabela zbiera dokładne dane liczbowe dla tego kluczowego testu, pozwalając na precyzyjne porównania.

\begin{table}[H]
    \centering
    \caption{Szczegółowe wyniki dla N=20000 (Dane losowe)}
    \label{tab:wyniki_n20000}
    \begin{tabular}{lrrr}
        \toprule % Ładna linia górna
        \textbf{Algorytm} & \textbf{Czas (ms)} & \textbf{Porównania} & \textbf{Przypisania} \\
        \midrule % Ładna linia środkowa
        InsertionSort     & 221 & 99563280 & 99583287 \\
        ModInsertionSort  & 221 & 99563280 & 99583289 \\
        MergeSort         & 4 & 287232 & 574464 \\
        ModMergeSort      & 2 & 272960 & 301320 \\
        HeapSort          & 4 & 510730 & 268324 \\
        HeapSortTernary   & 3 & 493320 & 180392 \\
        \bottomrule % Ładna linia dolna
    \end{tabular}
\end{table}

% ------------------------------------------------------
\section{Wnioski}
% ------------------------------------------------------

Przeprowadzone testy i analiza wykresów pozwoliły na sformułowanie następujących, kluczowych wniosków:

\begin{enumerate}
    \item Klasa złożoności rzeczywiście ma duże znaczenie. Różnica między $O(n^2)$ a $O(n \log n)$ jest gigantyczna. Algorytmy InsertionSort nadają się tylko do bardzo małych danych.
    
    \item Algorytmy "odporne" są bezpieczne. MergeSort i HeapSort działały tak samo szybko, nieważne czy dane były losowe, czy posortowane odwrotnie. To ich duża zaleta.
    
    \item InsertionSort ma swoją niszę. Mimo że jest ogólnie wolny, to dla danych już posortowanych (albo prawie posortowanych) jest najszybszy ze wszystkich, bo działa w czasie liniowym.
    
    \item Modyfikacje działają! Moje testy pokazały, że:
    \begin{itemize}
        \item ModMergeSort faktycznie zmniejszyło liczbę porównań i czas wykonania.
        \item HeapSortTernary mocno zmniejszył liczbę przypisań (przesunięć danych), co również przełożyło się na krótszy czas.
        \item Modyfikacja InsertionSort nie dała żadnego zysku na danych losowych.
    \end{itemize}

    \item Nie ma jednego "najlepszego" algorytmu. To, który jest najlepszy, zależy od sytuacji. Jeśli dane są prawie posortowane, InsertionSort sprawuje się najlepiej. Jeśli zależy nam na jak najmniejszej liczbie przesunięć danych, HeapSortTernary był świetny.
\end{enumerate}

\end{document}
